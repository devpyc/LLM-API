{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf14755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: torchaudio in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1f7ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.53.1-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from huggingface_hub) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (2025.7.9)\n",
      "Using cached transformers-4.53.1-py3-none-any.whl (10.8 MB)\n",
      "Using cached huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface_hub, tokenizers, accelerate, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.8.1 hf-xet-1.1.5 huggingface_hub-0.33.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.2 tqdm-4.67.1 transformers-4.53.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9a325",
   "metadata": {},
   "source": [
    "## 토근 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aa2a26",
   "metadata": {},
   "source": [
    "## torch 설치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a753e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 버전: 2.7.1\n",
      "MPS 가능: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch 버전:\", torch.__version__)\n",
    "print(\"MPS 가능:\", torch.backends.mps.is_available())\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bea02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lang Chain Framework 모델 불러와서 추론해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ec17f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕? 오늘 기분 어때?\"\n",
      "\"내일 기분 나빴잖아.\"\n",
      "\"이렇게 피곤해 보여. 괜찮으면 나 혼자만이라도 와. 내일도 피곤하니까 좀 쉬어.\"\n",
      "\"나 오늘도 피곤해 보여. 오늘은 피곤하니까 나가.\"\n",
      "\"내가 오늘은 나 혼자만이라도 와. 나 혼자만이라도 와.\"\n",
      "\"아휴, 나 혼자만이라도 와. 내가 내일도 피곤하니까 나도 같이 와.\"\n",
      "\"아휴, 나 혼자만이라도 와.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"안녕? 오늘 기분 어때?\"\n",
    "output = generator(prompt, max_new_tokens=100, temperature=0.7, do_sample=True)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:934: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/tf/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828f82010ade42298c68f6056c4e95f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bab2a25277f4eeab4414cb3772445c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb03d94740049b5ad89f3927e1d6af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc302754a4ea4244a425a5f06fac26cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92e72b243924d1faf1a132971be9c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf6857c68ed4bb292f8e3b57b5bb2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/180 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"Bllossom/llama-3.2-Korean-Bllossom-3B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\", use_auth_token=True)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"너는 날씨 전문가야. 오늘 안산과 울산의 날씨를 간단하게 알려줘.\"\n",
    "\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27e1fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c07fcd0e6b149e8836d10e883a3d1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84a8e9e7ef44a31b5a55ff36cc09e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732f159b68e4f32ac4300949916e1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mskt/kogpt2-base-v2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m generator = pipeline(\u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer)\n\u001b[32m     14\u001b[39m prompt = \u001b[33m\"\u001b[39m\u001b[33m안녕? 오늘 기분 어때?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/transformers/modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/transformers/modeling_utils.py:4747\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4742\u001b[39m         state_dict = load_gguf_checkpoint(checkpoint_files[\u001b[32m0\u001b[39m], return_tensors=\u001b[38;5;28;01mTrue\u001b[39;00m, model_to_load=dummy_model)[\n\u001b[32m   4743\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4744\u001b[39m         ]\n\u001b[32m   4746\u001b[39m     \u001b[38;5;66;03m# Find the correct dtype based on current state\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4747\u001b[39m     config, torch_dtype, dtype_orig = \u001b[43m_get_torch_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4748\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\n\u001b[32m   4749\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4751\u001b[39m config.name_or_path = pretrained_model_name_or_path\n\u001b[32m   4753\u001b[39m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/transformers/modeling_utils.py:1345\u001b[39m, in \u001b[36m_get_torch_dtype\u001b[39m\u001b[34m(cls, torch_dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only)\u001b[39m\n\u001b[32m   1343\u001b[39m     torch_dtype = get_state_dict_dtype(state_dict)\n\u001b[32m   1344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1345\u001b[39m     state_dict = \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m     torch_dtype = get_state_dict_dtype(state_dict)\n\u001b[32m   1349\u001b[39m logger.info(\n\u001b[32m   1350\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSince the `torch_dtype` attribute can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be found in model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms config object, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1351\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwill use torch_dtype=\u001b[39m\u001b[38;5;132;01m{torch_dtype}\u001b[39;00m\u001b[33m as derived from model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms weights\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1352\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/transformers/modeling_utils.py:556\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/langchain/lib/python3.11/site-packages/transformers/utils/import_utils.py:1517\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m():\n\u001b[32m   1516\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1518\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1519\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1520\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1521\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1522\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4f2f5cf76a407ea882116bc2bac517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"안녕? 오늘 기분 어때?\"\n",
    "output = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
